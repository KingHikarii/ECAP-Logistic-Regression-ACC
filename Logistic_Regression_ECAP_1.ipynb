{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Explained\n",
        "\n",
        "Logistic Regression is a statistical model that was developed in the field of statistics for binary classification problems. It gives an output of either 0 or 1.\n",
        "\n",
        "It was developed in order to calculate the growing population rate within an enviroment that has limited resources.\n",
        "\n",
        "Logistic Regression gets its name from its core function which is called as either the logistic function or the sigmoid function. The logistic function is an S-shaped curve that can take any real number and map it unto a value between 0 and 1 but never at those limits.![Logistic Regression.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAK+UExURf///9LS0szMzEZGRgAAAAEBAfj4+HJychgYGJ6enhISEi8vL8rKykBAQDo6Ovb29m1tbZubmwMDA0VFRcjIyDAwMEtLS/T09F5eXlxcXL6+vsnJyWNjY1FRUa6urkpKSsbGxtjY2FVVVRcXF3d3dwICApqamlRUVDk5OdDQ0CAgIFpaWqWlpe7u7pSUlKSkpJ2dnYuLi1tbWwoKCkFBQXh4eIqKipWVlZiYmM/Pz0dHR4SEhPLy8n9/f0JCQoKCgmhoaHR0dJKSkpmZmdXV1ZCQkHBwcFBQUBEREd7e3mZmZl9fX2pqaoeHhw4ODjU1NY6OjoCAgDExMefn5zw8PAcHB9HR0bS0tB0dHWdnZ7KysnZ2dgYGBhwcHIyMjDQ0NOXl5Q0NDVhYWOjo6MLCwjIyMj09PZeXl+Li4v39/bi4uDMzM319fWlpaQ8PD9ra2pycnCUlJRQUFNPT0wgICJOTk+Tk5KysrKGhoVJSUrm5ubOzswsLC+zs7GxsbNnZ2WJiYnFxcbe3t/r6+oaGhuPj41NTU7+/v6mpqXt7e2BgYODg4MXFxc3NzQUFBTs7O/f397u7uxYWFtTU1PPz8ywsLCcnJ/Hx8RoaGvn5+cPDw0NDQ4WFhc7OzldXV/v7+1lZWU9PT4mJienp6YiIiMHBwW5ubklJSSIiIh4eHqKioigoKObm5qqqqrGxsbCwsHV1dYODgyMjIxsbG93d3VZWVisrKykpKcDAwNbW1q+vrz8/P4GBgba2tgwMDKampnl5eaurqyoqKkxMTK2trcfHxzY2NpaWlnx8fJ+fn/7+/vX19R8fH7y8vMTExKenp4+Pj42Njfz8/BAQECYmJnNzc2VlZSEhIaCgoE5OTiQkJL29vdzc3OHh4aioqBUVFaOjoxMTE3p6ehkZGWtra+rq6khISNfX19/f37W1tYiVbRMAAAAJcEhZcwAADsIAAA7CARUoSoAAAAhaSURBVHhe7d35mxTVGQXgQj9xQAYEQhQXRBxREYKMyIAbAVkMIC4oigFiImFc4oiKg4hLJIKIoDEEcAtBRRRFkQiJuGcVVDCKa9yjcfkvfO53q4eq011dfbt7+taT57y/TPepW113znTPdFXXdAcBEREREREREREREREREREREREREREREREREREREREREWVXhw6YUIK99pZ9MKOCOu4rInWYUiGdOouI7Icx5etSb6oS6YoLCHXb31YlUo+LCHTvketKeuIyivlBr7aqRH6ISynigANNR70POvgQLetQXE579DENHdY3CILDtax+OIAi9pEjGuylI1lWmv5HdQwv6W95llUaluWAZTlgWQ5YlgOW5YBlOWBZDliWA5blgGU5YFkOWJYDluWAZTlgWQ5YlgOW5YBlOWBZDliWA5blgGU5YFkOWJYDLeswTKkgffn+aEypkGNMVzIAYyrkWC1LBmJOeQbtbbuSzj/CRbTH4OOGNDQeHVZlHD/0hGFNw0fgOAqC4MRITxEn4TgKgqAn1mSxrEJOLgyHERERERERERERERERUVWckj04xezAF/wyAKeYHfhjzQCcIhEREREREREREREREREREZF/p+pJICODoJ/5+mNcTDGjRGR0eOE0XJhRY8aOG9/r9J9gXAMTRCaar5PkDBtMPrP3CWeZTzzKphFnn2PPiSqhrCkdDsCoQufKeVOD4Py2x6CdygXTLoRx2fDT8PSx6TPSy5p5nn3QVNFUkZ8FF/383Nz1AUecbufzi4vjAzNg1i/NxGY3X3IpLsl32eUiMhjTMnT5laVXmkWu6NVySWTxlXOuMpO6uhqbqqZr5pppXYtxIa0T9Qc+D/MyXBfeme21+SJyfXxAxwm6fEE89e1MM6fumBYyJPwGq/ENxMs6+QaZfiMOucks73wzxj792kzpFkwLWKgPDAPuA+X5jWWv3FrwfQIbzcay9I549m3nFmGcZ9HiXFUit+HCSi2R25eKzMT4Dt3aEoz90c+cTb9jnb2nKpFluLRCy+9cHAR3iczCBUeZrQ3F1Jvf3mDmczfGoM/vzPOephX9W7SsLri8Mr9fuWp1ECy6R1beC0vO183dB6k3TWY2x2MK7jdjHjCXxuns/4ADKrNGFpovg/PfC/aP083mDoTUm7VmNg9iCh4SmfiwXlpX/bIGLRV5xFxYKCLr4aneo2ZzV8Uzfx4zs0l9r7kNj4cXHkova56BYRF6i32C4Aq98ER84UwNN8ZDXxbpZJ7EOFEJZc02IzAs1325KrNgnk7mGIwTVaWse8c81WnTrFaMC5mi23saYz8adDLdME5UeVmtm/+ktyHSNfXRHwSB/vk9EVM/HjRzeQbTZJWWtWVr2JRalf67zey3y58x9eMvZi77YZqswrL6zhCRyQ3Ptm6bV6+3dB2OQGvMqBZM/dADDvdjmqyysp4zT5ueDw+ZvaA39SKOAfbQQybe7rSbTmUrxskqKqv1JbNf3HaY9RAz8Jz4kDxP6wYzceRho05lGsbJKipLDyK83HbV7sI3x4bkOUkH6ZNW37bpVF7BOFklZf3VLPhbJPi7CVJ+B1yrG/wHxj78U6dyK8bJKinrX2bBq5Fgs97YHZEk3+M6JhNvxX+9TuUsjJNVUNbhuuq2SGK3Xvxwld4dpRPGPtipRH/aKRLKenj7HhvMiMj1pnDMGbpqdC27r1X86fkmHZPyi602duhUjsM4WUJZyzUubE04Rh+FsQ8CeU2XFz+2Z49ojcXYh1qWpR8x0/P1KF3+BtxW3GU6ZhjGPtSwrIsxz7kTbyxmp47ZjLEPVStrboQZEbkaljUiWlAM3ljMszrEvhTrmS1rDsbJEsqKSvhr+KKuOTSnXj1vhK+GFbZLV3N4Jth+bFnDMU5Wfllv6prPYZzGlpWJt6WxZTl8nlD5Zdl71g6M09iyHJ42tx9blsMf5vLL6qtr/hvjNLastzD2wZY1BONk5Zc1Rtd0fjzZsko6a6W9PalTCc+3K0X5Zb2tz7MexTjN3brBkk5baW9278zhWUz5Zdln8LsxTbNAN5h7Jc4re9ShAeNkFZRlXxd0PTJl916L72zXiD2e5XA6tS0r7wSOqKSy7F7efEgPgutojq6VdvC5JuyR0rUYJ7NlPYVxVFJZwRu67q5YNirtZG57/txyjH2wx+DzzyJLZD8Xs+g3mFjWdl13crQtE+lJIYle0ZUuwtgL3ZG7C9NEy3TqshLzqNl1dXV1GCr9FS8tbc+B33xHRN6Nj0Hv6TolnBlcA0+YqazCNMnO3ImS46fiolLY50wi709aN/CDYRM6m8tpf1wONYNmYOrHh2YuPTDN90HTyEkfPhN+s8Z/GkeOHu268/LR1ZFbUP1xCNIHdVdM/XhXp/wxxugT/CZDOC7NCr2jtNl/Cw7IU2fGNWLqx6c66eK/Y6tYVhBsGv9ZuO49o9LP+g0G6dBM7O3kzgdOnwx+emgIh5Wkw4Lh298auALjgg7W+VX5HNayDTCTcXj9vsY+N9Obi6kvek7pF5hmhp4PUY+pL816P8/s//aZU5Rcjoq0r7d3m+n8F+OM6KI/SnuidBboOT1fYpoRt5jJ6X+5ZsOV+sP7CuNM+F8PM7eixzhqTP+L9cvVGGfBejM1h7PHauBrva9nsC39d46M7OrkbNR9vg2bMPftKzOtl77B2LOP9RQ8Wdt84ZTXcJkXl36ybMk0PSgx/1tc5t9YbctI3aeuhdxkdjucWFBDgwb2/iJjZbUsLXr02rO9vtvxQmMmylq8ft3L1f5vWSIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIvq/9D0EFeDJNvKnVgAAAABJRU5ErkJggg==)"
      ],
      "metadata": {
        "id": "zEzRt9nIyt-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Representation Behind Logistic Regression**\n",
        "\n",
        "It is just like the linear regression where the equation is its representation, the input values (x) are combined linearly using weights to predict an output (y). However the key difference is the predicted output will give us a binary value 0 or 1 rather than a continuous numerical value. The logistic regression equation can be expressed as the following: ![Logistic Regression Equation.png](data:image/png;base64,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)\n",
        "\n",
        "*   P(y = 1|x) represent the probability of the positive class.\n",
        "*   The betas are the model's coefficients (weights) learned during training, while B0 is the bias\n",
        "*   The (x)s are the input features."
      ],
      "metadata": {
        "id": "xUi9kyldql5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Predicts Probabilities**\n",
        "\n",
        "Logistic regression models the probability of the default class (e.g the 1st class). For example, if we modelling people's sex as male or female from their height, then we can make the default class a male and the other class as female. Logistic regression will then predict that a given person is male baased off their height. This can then be written as **P ( y = 1 | height )**, where 1 is male and 0 is for female.\n",
        "\n",
        "In conclusion, logistic regression predicts probabilities. It is used for classification and estimating probabilities. It is a powerful classification technique technique that estimates the likelihood of an input belonging to a partificular class.\n",
        "\n",
        "**Learning the Logistic Regression**\n",
        "\n",
        "In logistic regression, we use the maximum likelihood to estimate the coefficients (beta values B) from our training input. If our model is predicting a value very close to 1 for the default class and a value close to 0 for the other class, then it means we are getting better coefficients.\n",
        "\n",
        "*The intuition for maximum likelihood* for logistic regression states this:\n",
        "\n",
        "\"The search procedure seeks values for the coefficients that minimizes the errors in the probabilities predicted by the model.\""
      ],
      "metadata": {
        "id": "p3ewOZPbuR1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Code"
      ],
      "metadata": {
        "id": "q9jn7TkCwaN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5rGOr9NzyirU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        y_predicted_cls = [1 if i >= 0.5 else 0 for i in y_predicted]\n",
        "        return y_predicted_cls\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "bc_data = load_breast_cancer()\n",
        "\n",
        "X, y = bc_data.data, bc_data.target\n",
        "\n",
        "#display(bc_data)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "classification = LogisticRegression(lr=0.01)\n",
        "classification.fit(X_train, y_train)\n",
        "predictions = classification.predict(X_test)\n",
        "\n",
        "def accuracy(predictions, y_test):\n",
        "    return np.sum(predictions == y_test) / len(y_test)\n",
        "\n",
        "acc = accuracy(predictions, y_test)\n",
        "print(acc)\n",
        "\n",
        "display(predictions[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "-E8An9R91q1N",
        "outputId": "cb73a7df-d511-48af-a30c-f6f3aebb2a34"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9210526315789473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-2fce6811a2c1>:33: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 0, 1, 0, 0]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Algorithm**\n",
        "\n",
        "**Developing the Gradient Descent**\n",
        "\n",
        "1. Within the init function, initialize the learning rate, and the number of iterations. The weight and bias should be set a None.\n",
        "\n",
        "2. Within the fit, which is also gradient descent algorithm, do the following:\n",
        "\n",
        "\tI. Set the number of features.\n",
        "\n",
        "\tII. Set the initial weights and bias as 0.\n",
        "\n",
        "3. After doing the second step, with the same fit, develop a for loop that will:\n",
        "\n",
        "\tI. Receive the predictions from the sigmoid function.\n",
        "\n",
        "\tII. Update gradients of the cost function.\n",
        "  \n",
        "\tIII. Update the value for the weights and bias by multiplying the learning rate with the gradients of the cost functions from II.\n",
        "\n",
        "4. Create a function that will return a binary number of either 1 or 0 based off the results from the predictions given by the sigmoid function\n",
        "\n",
        "**Within the Global Function**\n",
        "\n",
        "1. Receive the data.\n",
        "\n",
        "2. Set the features (x) and target variables (y).\n",
        "\n",
        "3. Perform the train and test set split from the features (x) and the target variable (y)\n",
        "\n",
        "4. Fit the X_train and y_train data so that they will go through the training of the classification model.\n",
        "\n",
        "5. Use the fitted model to predict the labels for the test data of X_test. Labels meaning the target values or categories that the model is trying to predict.\n",
        "\n",
        "6. Calculate the accuracy.\n"
      ],
      "metadata": {
        "id": "3a3dMbzjGYS5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6t5PyB7MDyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}